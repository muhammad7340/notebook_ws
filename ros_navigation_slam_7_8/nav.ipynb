{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ce3a93",
   "metadata": {},
   "source": [
    "### **SLAM** (Simultaneous Localization and Mapping)\n",
    "\n",
    "SLAM is a technique used in robotics and computer vision to create a **`map`** of an unknown environment while simultaneously keeping track of the **`agent's location`** within that environment. It combines data from various sensors, such as cameras and LiDAR, to build a 2D (grid maps) or 3D map (3D point cloud) and localize the robot within it.\n",
    "\n",
    "Two main techniques used in SLAM are:\n",
    "1. Build a map of an unknown environment (mapping).\n",
    "2. Track its own pose (position and orientation) within that map at the same time (localization).\n",
    "\n",
    "Core functionalities of Slam Algorithms:\n",
    "\n",
    "1. `Sensor inputs`\n",
    "- SLAM typically uses sensor data (e.g., LIDAR scans, camera images, or depth sensor measurements) to detect features or landmarks in the environment.\n",
    "2. `State estimation`\n",
    "- An internal state (the robot’s pose, including x, y, yaw) is estimated using algorithms like Extended Kalman Filters, Particle Filters, or Graph Optimization.\n",
    "3. `Map building`\n",
    "- As the robot moves, it accumulates new sensor data. The SLAM algorithm integrates that data into a global map (2D grid map, 3D point cloud, or other representations).\n",
    "4. `Loop closure`\n",
    "- When the robot revisits a previously mapped area, the SLAM algorithm detects that it’s the same place (loop closure). This knowledge is used to reduce accumulated drift and refine both the map and pose estimates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Slam toolbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefb1b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt install ros-jazzy-slam-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397b172",
   "metadata": {},
   "source": [
    "There is a new node in the launch file marker_server from the `interactive-marker-twist-server` this can be used to `move and rotate our robot directly from **RViz***`. Let's install it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ce3f7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt install ros-jazzy-interactive-marker-twist-server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546548d",
   "metadata": {},
   "source": [
    "We moved all the `parameter_bridge` topics into a yaml config file `gz_bridge.yaml`, we don't describe them in the launch file anymore, this is more comfortable when we forward many topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518b8bd",
   "metadata": {},
   "source": [
    "`base_link` → robot’s body center.\n",
    "\n",
    "`odom` frame → comes from wheel encoders (`local` odometry frame).\n",
    "- It estimates how much your `robot moved` — but it `drifts` over time (small errors add up).\n",
    "\n",
    "`map` frame → comes from SLAM (e.g. from LiDAR-based mapping, `Global` reference frame).\n",
    "- It constantly `corrects` that drift by `matching sensor data with the map`.\n",
    "\n",
    "\n",
    "So, if your odometry drifts, the SLAM algorithm shifts the robot’s map pose to keep it consistent with real-world features.\n",
    "The difference between map and odom = how much drift your raw odometry accumulated since SLAM had to correct it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fa8c8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "---\n",
    "#### **MAPPING**\n",
    "\n",
    "\n",
    "SLAM tool box offers 2 ways to store maps:\n",
    "\n",
    "1. Save Map  \n",
    "- Command: `ros2 run nav2_map_server map_saver_cli -f my_map`  \n",
    "- Output:  \n",
    "  - `my_map.pgm` → grayscale image (white = free, black = occupied)  \n",
    "  - `my_map.yaml` → metadata (resolution, origin, etc.)  \n",
    "- Use: for localization only later (e.g. `nav2_amcl`)  \n",
    "- Limitation: cannot resume mapping since it's just an image  \n",
    "- Analogy: like taking a screenshot of your map  \n",
    "\n",
    "2. Serialize Map  \n",
    "- Command: `ros2 service call /slam_toolbox/serialize_map ...`  \n",
    "- Saves SLAM Toolbox’s internal graph (poses, scans, loop closures, etc.)  \n",
    "- Allows later deserialization to continue mapping from the same state  \n",
    "- Use: for continuing SLAM sessions, not for localization with other nodes  \n",
    "- Analogy: like saving a project file to edit later  \n",
    "\n",
    "**Note:** In Gui windows type the name of both save and serialize files, and click once , they will be automatically saved in the *_ws folder, we must be in the *_ws folder during launch file.\n",
    "\n",
    "3. Deserialize Map  \n",
    "- Command: `ros2 service call /slam_toolbox/deserialize_map ...`\n",
    "- Loads a previously serialized map to resume SLAM\n",
    "- Use: to continue mapping from where you left off\n",
    "- Limitation: cannot use deserialized maps for localization with other nodes\n",
    "- Analogy: like opening a saved project file to continue working\n",
    "\n",
    "**Note:** Type name of any saved serialize_map without any extension (both .data and .posegraph files should be in the *_ws folder) in the Gui window and click once, it will be automatically loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec1b5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Custom deserialize map executable Working**\n",
    "\n",
    "1. MapLoaderNode which calls the service to deserialize:\n",
    "\n",
    "- Doesn’t open or parse the map itself. Calls a ROS2 service:\n",
    "`/slam_toolbox/deserialize_map`\n",
    "(service type slam_toolbox/srv/DeserializePoseGraph).\n",
    "\n",
    "- That service is implemented inside the slam_toolbox node, not by us. So, when our node sends this request:\n",
    "`self.request.filename = \"/root/nav_ws/install/ros2_nav/share/ros2_nav/maps/serialized\"`\n",
    "\n",
    "- the slam_toolbox node receives it, opens that serialized file (a .posegraph file), and reconstructs:\n",
    "   - its internal pose graph,\n",
    "   - the map data,\n",
    "   - and all the robot poses stored inside it.\n",
    "\n",
    "- Essentially, slam_toolbox “loads” the map into its running memory, not into another file.\n",
    "\n",
    "2. Once slam_toolbox finishes deserializing:\n",
    "\n",
    "- It populates its internal occupancy grid, constraints, and graph nodes.\n",
    "- Then `it starts advertising topics` like:\n",
    "   - /map (OccupancyGrid)\n",
    "   - /map_metadata\n",
    "   - /slam_toolbox/pose_graph_visualization\n",
    "   - /slam_toolbox/map_updates\n",
    "- It also provides the /map -> /odom transform via TF2.\n",
    "\n",
    "That’s the `shared state` that `other launch files` (like navigation or localization launch) use. they don’t reload the serialized file themselves.\n",
    "Other `launch files/nodes` just `connect to those ROS2 topics/services`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e77666",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### **LOCALIZATION**\n",
    "\n",
    "While mapping was the process of creating a representation (a map) of an environment. Localization is the process by which a `robot determines its own position and orientation within a known environment (map)`. In other words:\n",
    "\n",
    "- The environment or map is typically already available or pre-built.\n",
    "- The robot’s task is to figure out “Where am I?” or “Which direction am I facing?” using sensor data, often by `matching its current perceptions to the known map`.\n",
    "\n",
    "\n",
    "#### Localization with AMCL\n",
    "`AMCL` (Adaptive Monte Carlo Localization) is a `particle filter–based 2D localization algorithm`. The robot’s possible `poses` (position + orientation in 2D) are represented by a `set of particles`. It adaptively samples the robot’s possible poses according to sensor readings and motion updates, converging on an accurate estimate of where the robot is within a known map.\n",
    "\n",
    "AMCL is part of the ROS2 navigation stack, let's install it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f2caa",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt install ros-jazzy-nav2-bringup \n",
    "sudo apt install ros-jazzy-nav2-amcl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6572",
   "metadata": {},
   "source": [
    "Write a separate launch file for localization, name it `localization.launch.py` in the `launch` folder of your package. include rviz_config_arg, rviz_launch_arg, sim_time_arg, nav2_localization_launch_path (Nav2 bringup),  localization_params_path(amcl_localization.yaml), map_file_path (my_map.yaml), rviz_node, sim_time_arg,    interactive_marker_twist_server_node, localization_launch.\n",
    "\n",
    "\n",
    "[amcl_localization.yaml](https://docs.nav2.org/configuration/packages/configuring-amcl.html) (Link from official doc of Nav2). We have to set this for our localization.launch.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607794cb",
   "metadata": {},
   "source": [
    "[Nav2 Documentation](https://docs.nav2.org/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90229d3f",
   "metadata": {},
   "source": [
    "The `main purpose` of the `localization algorithm` is establishing the `transformation` between the `fixed map` and the `robot's odometry frame` based on `real time sensor data`. We can visualize this in RViz as we saw it during mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512a5e5",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578079bb",
   "metadata": {},
   "source": [
    "#### Correlation vs Covariance \n",
    "- Correlation describe the direction and strength of relationship between two Random variables.\n",
    "- Covariance indicates the extent/direction to which two random variables change together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11009ea2",
   "metadata": {},
   "source": [
    "<img src=\"assets/c1.png\" width=\"800\" height=\"600\"/>\n",
    "\n",
    "\n",
    "**`Population`**\n",
    "All possible data points in a group you want to study.\n",
    "Example: the heights of all students in a university.\n",
    "\n",
    "**`Sample`**\n",
    "A smaller subset taken from the population to estimate its characteristics.\n",
    "Example: heights of 50 students selected from that university.\n",
    "\n",
    "<img src=\"assets/c2.png\" width=\"750\" height=\"600\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b127d",
   "metadata": {},
   "source": [
    "<img src=\"assets/c3.png\" width=\"375\" height=\"200\"/>\n",
    "<img src=\"assets/c4.png\" width=\"375\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf37a66",
   "metadata": {},
   "source": [
    "<img src=\"assets/c5.png\" width=\"750\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad2483",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bcc65",
   "metadata": {},
   "source": [
    "<img src=\"assets/c6.png\" width=\"750\" height=\"300\"/>\n",
    "<br>\n",
    "<img src=\"assets/c7.png\" width=\"750\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b8f8a",
   "metadata": {},
   "source": [
    "<img src=\"assets/c8.png\" width=\"600\" height=\"200\"/>\n",
    "<br>\n",
    "<img src=\"assets/c9.png\" width=\"600\" height=\"550\"/>\n",
    "<br>\n",
    "\n",
    "**`Variance`**\n",
    "- Measures how far each data point deviates from the mean on average (spread of data).\n",
    "- Example: if exam scores are 70, 80, 90, the variance shows how far each score lies from the mean 80.\n",
    "\n",
    "**`Standard deviation`**\n",
    "- It is the square root of variance, giving spread in the same unit as the data.\n",
    "- Example: if variance of exam scores is 25, then standard deviation is 5, meaning scores typically differ by 5 marks from the mean.\n",
    "\n",
    "<img src=\"assets/c10.png\" width=\"600\" height=\"450\"/>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef88ef2",
   "metadata": {},
   "source": [
    "#### Covariance Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72e10b",
   "metadata": {},
   "source": [
    "<img src=\"assets/c11.png\" width=\"750\" height=\"600\"/>\n",
    "<br>\n",
    "<img src=\"assets/c13.png\" width=\"750\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468036d",
   "metadata": {},
   "source": [
    "<img src=\"assets/c14.png\" width=\"750\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3672de",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84c881",
   "metadata": {},
   "source": [
    "#### **Pose Estimation**\n",
    "- Initial Pose Calculation of Robot using Covariance Matrix\n",
    "\n",
    "\n",
    "AMCL needs the `initial pose` which can be given using `RviZ's` `2D Estimate Pose` tool. Then AMCL's particles start appearing on the RviZ, each particle represents a pose [x,y,yaw(0)]. \n",
    "\n",
    "We can define our own Custom node for `initial pose sending` and run using `ros2 run ros2_nav_py send_initial_pose` which will be a publisher node and publish at `/initialpose` which is required by AMCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf07507",
   "metadata": {},
   "source": [
    "<img src=\"assets/c15.png\" width=\"750\" height=\"550\"/>\n",
    "<img src=\"assets/c16.png\" width=\"750\" height=\"550\"/>\n",
    "<img src=\"assets/c17.png\" width=\"750\" height=\"600\"/>\n",
    "<img src=\"assets/c18.png\" width=\"750\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01899d71",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f5827",
   "metadata": {},
   "source": [
    "#### **Eulear Angles (rpy) vs Quaternions (qx,qy,qz,qw)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d1109",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"assets/q2.png\" width=\"750\" height=\"500\"/>\n",
    "\n",
    "<img src=\"assets/q6.png\" width=\"750\" height=\"500\"/>\n",
    "\n",
    "<img src=\"assets/q3.png\" width=\"750\" height=\"600\"/>\n",
    "\n",
    "<img src=\"assets/q1.png\" width=\"750\" height=\"600\"/>\n",
    "<img src=\"assets/q4.png\" width=\"750\" height=\"500\"/>\n",
    "<img src=\"assets/quaternions.png\" width=\"750\" height=\"500\"/>\n",
    "<img src=\"assets/q5.png\" width=\"750\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b30611",
   "metadata": {},
   "source": [
    "- `Euler angles` use `sequential rotations` and suffer from `singularities` at specific configurations.\n",
    "- `Quaternions` use a `unified 4D representation` based on axis–angle formulation, preserving smoothness and numerical stability across all possible orientations.\n",
    "\n",
    "Links: (also in assets folder)\n",
    "\n",
    "[Eulear Angles](https://danceswithcode.net/engineeringnotes/rotations_in_3d/rotations_in_3d_part1.html)\n",
    "\n",
    "[Quaternions](https://danceswithcode.net/engineeringnotes/quaternions/quaternions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e4b85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00e907",
   "metadata": {},
   "source": [
    "#### **Localization with SLAM**\n",
    "\n",
    "It's possible to use SLAM toolbox in `localization mode`, it requires a small adjustment on the parameters which is already part of this lesson, slam_toolbox_localization.yaml. This requires the path to the serialized map file.Make sure the map_file_name parameter is changed to the path on your machine to the serialized map file! Create the launch file for localization, localization_slam_toolbox.launch.py, it's very similar to the SLAM toolbox mapping launch file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789405f5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Rebuild the workspace and try it, we'll see that the map keeps updating unlike with AMCL, this is the normal behavior of the localization with SLAM toolbox. According to this [paper](https://joss.theoj.org/papers/10.21105/joss.02783), the localization mode does continue to update the pose-graph with new constraints and nodes, but the updated map expires over some time. SLAM toolbox describes it as \"elastic\", which means it holds the updated graph for some amount of time, but does not add it to the permanent graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10262e",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### **Navigation**\n",
    "Navigation in robotics is the overall process that enables a robot to move from one location to another in a safe, efficient, and autonomous manner. It typically involves:\n",
    "\n",
    "- Knowing where the robot is (`localization or SLAM`),\n",
    "- Knowing where it needs to go (a `goal pose or waypoint`),\n",
    "- Planning a path to reach that goal (`path planning`), and\n",
    "- Moving along that path while avoiding dynamic and static obstacles (`motion control` and `obstacle avoidance`).\n",
    "\n",
    "ROS's nav2 navigation stack implements the above points 2 to 4, for the first point we already met several possible solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ac3ed",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee53ae",
   "metadata": {},
   "source": [
    "#### **Navigation with SLAM**\n",
    "\n",
    "As we saw to navigate a mobile robot we need to know 2 things:\n",
    "\n",
    "- Knowing where the robot is (localization or SLAM),\n",
    "- Knowing where it needs to go (a goal pose or waypoint)\n",
    "In the previous examples we used `localization on a known map`, but it's also possible to navigate together with an `online SLAM`. It means we don't know the complete environment around the robot but we can already navigate in the known surrounding.\n",
    "\n",
    "Create a navigation_with_slam.launch.py launch file where we start both the `SLAM toolbox` and the `navigation stack`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f9c4a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### **Exploration**\n",
    "Exploration is a process by which a robot operating in an `unknown` or partially known `environment` actively `searches` the space to gather new information. This is a real life use case to use SLAM together with the navigation stack.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/MOGI-ROS/m-explore-ros2.git\n",
    "```\n",
    "Build the workspace and source the setup.bash to make sure ROS is aware about the new package!\n",
    "\n",
    "We'll need 3 terminals, one for the simulation:\n",
    "```bash\n",
    "ros2 launch bme_ros2_navigation spawn_robot.launch.py\n",
    "```\n",
    "In another terminal we launch the navigation_with_slam.launch.py:\n",
    "```bash\n",
    "ros2 launch bme_ros2_navigation navigation_with_slam.launch.py\n",
    "```\n",
    "And in the third one we launch the exploration:\n",
    "```bash\n",
    "ros2 launch explore_lite explore.launch.py\n",
    "```\n",
    "The exploration node will identify the boundaries of the know surrounding and will navigate the robot until it finds all the physical boundaries of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c9aa8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd1012a",
   "metadata": {},
   "source": [
    "Custom camera_subscriber and robot_mover nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fd854",
   "metadata": {},
   "source": [
    "\n",
    "Main Topics and Nodes:\n",
    "\n",
    "\n",
    "Use commands: \n",
    "- `ros2 topic list` to list all topics.\n",
    "- `ros2 topic list -t | grep </camera>` to list all topics of named /camera (filtered output).\n",
    "- `ros2 topic info <topic_name> -v ` to get details about a specific topic with verbose output.\n",
    "- `ros2 topic echo <topic_name> --once` to see the messages being published on a topic in real-time only once.\n",
    "\n",
    "\n",
    "\n",
    "1. **`/camera/image`**\n",
    "\n",
    "- *Publisher*: Usually a camera driver node (like realsense_camera, usb_cam, gazebo_ros_camera, or image_publisher, `ros_gz_image` (This node is part of the Gazebo–ROS bridge system. It takes simulated camera frames from Gazebo and publishes them into ROS2 as sensor_msgs/msg/Image.)).\n",
    "- *Message type*: sensor_msgs/msg/Image → raw pixel array + metadata.\n",
    "- *Subscribers*: Vision or navigation nodes (rviz2, object detector, visual SLAM, etc.).\n",
    "- *Used for*: Perception, object detection, mapping, SLAM, etc.\n",
    "\n",
    "\n",
    "2. **`/camera/image/compressed`**\n",
    "- *Publisher*: Usually a camera driver node (like realsense_camera, usb_cam, gazebo_ros_camera, or image_publisher, `ros_gz_image` (This node is part of the Gazebo–ROS bridge system. It takes simulated camera frames from Gazebo and publishes them into ROS2 as sensor_msgs/msg/Image.)).\n",
    "- *Message type*: sensor_msgs/msg/CompressedImage → JPEG compressed image data.\n",
    "- *Subscribers*: Vision or navigation nodes (`rviz2` (here subscribe to it), object detector, visual SLAM, etc.).\n",
    "- *Used for*: Perception, object detection, mapping, SLAM, etc.\n",
    "\n",
    "\n",
    "3. **`/cmd_vel`**\n",
    "\n",
    "- *Publisher*: Something that generates motion commands could be:\n",
    "  - `teleop_twist_keyboard` (manual control), `collision_monitor`, `docking_server`\n",
    "  - nav2_controller or `bt_navigator` (autonomous navigation)\n",
    "- *Message type*: geometry_msgs/msg/Twist\n",
    "  - linear.x → forward/back speed (m/s). `Its not distance but speed.`\n",
    "  - angular.z → rotation speed (rad/s)\n",
    "- *Subscriber*: The robot’s base controller (e.g., diff_drive_controller, cmd_vel_mux, or ros2_control hardware interface).\n",
    "- Connection flow: `teleop_twist_keyboard → /cmd_vel → ros_gz_bridge → Gazebo robot base plugin`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0be70",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# run launch file to view camera frames in window and save after 30secs\n",
    "ros2 launch camera_access_pkg camera_subscriber.launch.py use_sim_time:=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4090c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0466f0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "#### Yollo v-11 in Gazebo\n",
    "\n",
    "Magic command:\n",
    "`find install -type f -executable -exec sed -i '1s|^#!.*python3$|#!/root/nav_ws/venv/bin/python3|' {} +`\n",
    "- Finds every executable file in install/.\n",
    "- Rewrites the first line (#!...python3) to point to your venv interpreter.\n",
    "- Fixes all ROS 2 entry points to use your venv’s Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import CompressedImage, Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics.nn import tasks\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "class YoloSegmentation(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('segmentation_node')\n",
    "        \n",
    "        # --- Model and Annotator Initialization ---\n",
    "        self.get_logger().info(\"Loading YOLOv11 model...\")\n",
    "\n",
    "        self.model = YOLO(\"yolo11n-seg.pt\")\n",
    "        self.mask_annotator = sv.MaskAnnotator(color=sv.ColorPalette.ROBOFLOW)\n",
    "        self.box_annotator = sv.BoxCornerAnnotator(color=sv.ColorPalette.ROBOFLOW)\n",
    "        self.label_annotator = sv.LabelAnnotator()\n",
    "        self.get_logger().info(\"Model loaded successfully.\")\n",
    "\n",
    "        # --- ROS2 Communication Setup ---\n",
    "        self.bridge = CvBridge()\n",
    "        self.subscription = self.create_subscription(\n",
    "            CompressedImage,\n",
    "            '/camera/image/compressed',\n",
    "            self.image_callback,\n",
    "            10)\n",
    "        \n",
    "        self.publisher = self.create_publisher(Image, '/yolo/segmented_image', 10)\n",
    "        \n",
    "    def image_callback(self, msg: CompressedImage):\n",
    "        try:\n",
    "            # 1. Decompress ROS message to OpenCV frame\n",
    "            np_arr = np.frombuffer(msg.data, np.uint8)\n",
    "            frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "            # 2. Perform inference\n",
    "            results = self.model(frame)[0]\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "            # 3. Create labels\n",
    "            labels = [\n",
    "                f\"{self.model.names[class_id]} {confidence:0.2f}\"\n",
    "                for confidence, class_id\n",
    "                in zip(detections.confidence, detections.class_id)\n",
    "            ]\n",
    "            \n",
    "            # 4. Annotate the frame\n",
    "            annotated_frame = self.mask_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "            annotated_frame = self.box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "            annotated_frame = self.label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "            # 5. Convert annotated frame back to ROS message and publish\n",
    "            annotated_msg = self.bridge.cv2_to_imgmsg(annotated_frame, \"bgr8\")\n",
    "            self.publisher.publish(annotated_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.get_logger().error(f\"Error processing image: {e}\")\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = YoloSegmentation()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f02950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import CompressedImage, Image\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from cv_bridge import CvBridge\n",
    "from ament_index_python.packages import get_package_share_directory\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import supervision as sv\n",
    "import time\n",
    "\n",
    "class YoloSegmentation(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('segmentation_node')\n",
    "        self.get_logger().info(\"Loading YOLOv11 ONNX model (CPU optimized)...\")\n",
    "\n",
    "        # --- ONNX Inference Session (CPU) ---\n",
    "        # Get absolute path inside installed package\n",
    "        pkg_path = get_package_share_directory('yollo_seg')\n",
    "        model_path = os.path.join(pkg_path, 'models', 'yolo11n-seg.onnx')\n",
    "        self.session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.output_names = [o.name for o in self.session.get_outputs()]\n",
    "        self.get_logger().info(\"ONNX model loaded successfully.\")\n",
    "\n",
    "        # --- ROS2 Communication ---\n",
    "        self.bridge = CvBridge()\n",
    "        self.processing = False  # skip-frame flag\n",
    "\n",
    "        self.subscription = self.create_subscription(\n",
    "            CompressedImage,\n",
    "            '/camera/image/compressed',\n",
    "            self.image_callback,\n",
    "            qos_profile_sensor_data)  # drop old frames automatically\n",
    "\n",
    "        self.publisher = self.create_publisher(Image, '/yolo/segmented_image', 10)\n",
    "\n",
    "        # --- Annotators (for visualization only) ---\n",
    "        self.mask_annotator = sv.MaskAnnotator(color=sv.ColorPalette.ROBOFLOW)\n",
    "        self.box_annotator = sv.BoxCornerAnnotator(color=sv.ColorPalette.ROBOFLOW)\n",
    "        self.label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "    def image_callback(self, msg: CompressedImage):\n",
    "        if self.processing:\n",
    "            return  # skip if still busy\n",
    "        self.processing = True\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # --- 1. Decompress image ---\n",
    "            np_arr = np.frombuffer(msg.data, np.uint8)\n",
    "            frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "            # --- 2. Preprocess: resize & normalize ---\n",
    "            resized = cv2.resize(frame, (320, 320))  # smaller = faster\n",
    "            img = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            img = np.transpose(img, (2, 0, 1))  # HWC → CHW\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "\n",
    "            # --- 3. Inference ---\n",
    "            outputs = self.session.run(self.output_names, {self.input_name: img})\n",
    "\n",
    "            # TODO: decode segmentation (optional — based on your exported model)\n",
    "            # For now, we’ll just republish a resized grayscale placeholder mask\n",
    "            seg_output = (np.mean(resized, axis=2)).astype(np.uint8)\n",
    "\n",
    "            annotated_msg = self.bridge.cv2_to_imgmsg(seg_output, \"mono8\")\n",
    "            self.publisher.publish(annotated_msg)\n",
    "\n",
    "            dt = time.time() - start_time\n",
    "            self.get_logger().info(f\"Frame processed in {dt:.3f}s ({1/dt:.2f} FPS)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.get_logger().error(f\"Error processing image: {e}\")\n",
    "        finally:\n",
    "            self.processing = False\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = YoloSegmentation()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d2da0",
   "metadata": {},
   "source": [
    "Yollo-11E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc314aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import CompressedImage, Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLOE  # <-- YOLOE import (new)\n",
    "import supervision as sv       # for visualization helpers\n",
    "\n",
    "\n",
    "class YoloESegmentation(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('yoloe_segmentation_node')\n",
    "\n",
    "        # --- Load YOLOE model ---\n",
    "        self.get_logger().info(\"Loading YOLOE-11S prompt-free segmentation model...\")\n",
    "        # use smallest, prompt-free version for best performance on edge\n",
    "        self.model = YOLOE(\"yoloe-11s-seg-pf.pt\")  \n",
    "        self.get_logger().info(\"Model loaded successfully.\")\n",
    "\n",
    "        # --- Visualization annotators (for masks, boxes, labels) ---\n",
    "        self.mask_annotator = sv.MaskAnnotator(color=sv.ColorPalette.ROBOFLOW)\n",
    "        self.box_annotator = sv.BoxCornerAnnotator(color=sv.ColorPalette.ROBOFLOW)\n",
    "        self.label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "        # --- ROS setup ---\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        # Subscribe to compressed image topic\n",
    "        self.subscription = self.create_subscription(\n",
    "            CompressedImage,\n",
    "            '/camera/image/compressed',\n",
    "            self.image_callback,\n",
    "            10\n",
    "        )\n",
    "\n",
    "        # Publish segmented annotated image\n",
    "        self.publisher = self.create_publisher(Image, '/yolo/segmented_image', 10)\n",
    "\n",
    "        self.get_logger().info(\"Subscribed to /camera/image/compressed\")\n",
    "\n",
    "    def image_callback(self, msg: CompressedImage):\n",
    "        try:\n",
    "            # 1. Convert ROS CompressedImage → OpenCV image\n",
    "            np_arr = np.frombuffer(msg.data, np.uint8)\n",
    "            frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "            if frame is None:\n",
    "                self.get_logger().warn(\"Received empty frame, skipping.\")\n",
    "                return\n",
    "\n",
    "            # 2. Run YOLOE inference (prompt-free segmentation)\n",
    "            results = self.model.predict(frame, verbose=False)[0]\n",
    "\n",
    "            # 3. Convert YOLO output → Supervision detections\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "            # 4. Prepare display labels\n",
    "            labels = [\n",
    "                f\"{self.model.names[class_id]} {confidence:.2f}\"\n",
    "                for confidence, class_id in zip(detections.confidence, detections.class_id)\n",
    "            ]\n",
    "\n",
    "            # 5. Annotate frame with masks + boxes + labels\n",
    "            annotated = frame.copy()\n",
    "            annotated = self.mask_annotator.annotate(scene=annotated, detections=detections)\n",
    "            annotated = self.box_annotator.annotate(scene=annotated, detections=detections)\n",
    "            annotated = self.label_annotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
    "\n",
    "            # 6. Convert back to ROS Image message and publish\n",
    "            img_msg = self.bridge.cv2_to_imgmsg(annotated, encoding=\"bgr8\")\n",
    "            self.publisher.publish(img_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.get_logger().error(f\"Error processing frame: {e}\")\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = YoloESegmentation()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        node.get_logger().info(\"Shutting down YOLOE segmentation node.\")\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e29f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Modelling actors in Gazebo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c3c7b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "<actor name=\"actor_walking\">\n",
    "  <!--basic minimal starting code-->\n",
    "  <!-- Starting pose, nice for when the world is reset -->\n",
    "  <!--this pose is different from next one in waypoints, so set this first then next-->\n",
    "  <pose>\n",
    "    -5.5\n",
    "    0.0\n",
    "    0.0\n",
    "    0.0\n",
    "    0.0\n",
    "    0.0\n",
    "  </pose>\n",
    "  <!-- Actor visual model -->\n",
    "  <skin>\n",
    "    <filename>package://ros2_nav/worlds/models/walk.dae</filename>\n",
    "    <scale>1.0</scale>\n",
    "  </skin>\n",
    "</actor>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef487417",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<actor name=\"actor_walking\">\n",
    "<!-- Starting pose, nice for when the world is reset -->\n",
    "  <!--this pose is different from next one, so set this first then next-->\n",
    "  <pose>\n",
    "    -5.5\n",
    "    0.0\n",
    "    0.0\n",
    "    0.0\n",
    "    0.0\n",
    "    0.0\n",
    "  </pose>\n",
    "  <!-- Actor visual model -->\n",
    "  <skin>\n",
    "    <filename>package://ros2_nav/worlds/models/walk.dae</filename>\n",
    "    <scale>1.0</scale>\n",
    "  </skin>\n",
    "  <!-- Actor animation -->\n",
    "  <animation name=\"walk\">\n",
    "    <filename>package://ros2_nav/worlds/models/walk.dae</filename>\n",
    "    <interpolate_x>true</interpolate_x>\n",
    "  </animation>\n",
    "    <script>\n",
    "      <loop>true</loop>\n",
    "      <delay_start>0.000000</delay_start>\n",
    "      <auto_start>true</auto_start>\n",
    "      <!--type must match the animation name-->\n",
    "      <trajectory id=\"0\" type=\"walk\">\n",
    "      <!--go straight, turn 360 and then comeback in reverse-->\n",
    "      <waypoint>\n",
    "        <time>0</time>\n",
    "        <pose>-1 0 1.0 0 0 0</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>2</time>\n",
    "        <pose>1 0 1.0 0 0 0</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>4</time>\n",
    "        <pose>3 0 1.0 0 0 0</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>6</time>\n",
    "        <pose>5 0 1.0 0 0 0</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>8</time>\n",
    "        <pose>7 0 1.0 0 0 0</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>8.5</time>\n",
    "        <pose>7 0 1.0 0 0 3.1416</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>10.5</time>\n",
    "        <pose>5 0 1.0 0 0 3.1416</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>12.5</time>\n",
    "        <pose>3 0 1.0 0 0 3.1416</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>14.5</time>\n",
    "        <pose>1 0 1.0 0 0 3.1416</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>16.5</time>\n",
    "        <pose>-1 0 1.0 0 0 3.1416</pose>\n",
    "      </waypoint>\n",
    "      <waypoint>\n",
    "        <time>16.5</time>\n",
    "        <pose>-1 0 1.0 0 0 0</pose>\n",
    "      </waypoint>\n",
    "      </trajectory>\n",
    "  </script>\n",
    "</actor>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172671b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#1. Rebuild to ensure correct permissions\n",
    "colcon build --packages-select yollo_seg --symlink-install\n",
    "\n",
    "#2. Source your workspace\n",
    "source install/setup.bash\n",
    "\n",
    "#3. If still failing, manually add execute permission\n",
    "chmod +x install/yollo_seg/lib/yollo_seg/segmentation_node\n",
    "\n",
    "#4. Verify it’s executable\n",
    "ls -l install/yollo_seg/lib/yollo_seg/segmentation_node\n",
    "\n",
    "#5. Relaunch\n",
    "ros2 launch yollo_seg yollo_seg.launch.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ed980",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### New models in .sdf files\n",
    "\n",
    "\n",
    "- check using: `ls /root/gz_sim/gazebo_models`\n",
    "- check using: `echo $GZ_SIM_RESOURCE_PATH`\n",
    "\n",
    "- See any breaks in .sdf file using: \n",
    "`xmllint /root/nav_ws/src/ros2_nav/worlds/new.sdf --noout`\n",
    "\n",
    "\n",
    "- Every time we save a gazebo file as world: Replace this using Ctrl+f with nothing:\n",
    "file:///root/nav_ws/install/ros2_nav/share/ros2_nav/worlds/\n",
    "\n",
    "- Remove this at end of the file, when added new models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eea470",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<uri>file://<urdf-string></uri> \n",
    "<name>mogi_bot</name> \n",
    "<pose>2.5000000000756399 1.500000000172296 0.09999915204843178 8.2075346820849609e-10 1.8688510693122136e-09 -1.570699999999998</pose> \n",
    "</include> <include> <uri>file:///root/gz_sim/gazebo_models/Table</uri> \n",
    "<name>Table</name> \n",
    "<pose>4.5043991416676885 -5.1928294349772566 0 0 0 0</pose> \n",
    "</include> \n",
    "<include> \n",
    "<uri>file:///root/gz_sim/gazebo_models/Office Chair</uri> \n",
    "<name>OfficeChair</name> \n",
    "<pose>4.7650436507482272 -6.3532852596903684 0 0 0 0</pose> \n",
    "</include>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0366b",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c76be6",
   "metadata": {},
   "source": [
    "Wall with scars of damage best\n",
    "\n",
    "![image](assets/wall_damage_scars.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a2d0e",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<material>\n",
    "  <pbr>\n",
    "    <metal>\n",
    "      <!-- Path to your downloaded texture -->\n",
    "      <albedo_map>package://ros2_nav/worlds/models/wall_gray_bullets.jpg</albedo_map>\n",
    "      <roughness>0.8</roughness>   <!-- matte finish -->\n",
    "      <metalness>0.05</metalness>   <!-- low reflectivity -->\n",
    "    </metal>\n",
    "  </pbr>\n",
    "\n",
    "  <!-- Stone-gray tone: cool gray with hint of beige/olive -->\n",
    "  <ambient>0.40 0.41 0.38 1</ambient>\n",
    "  <diffuse>0.45 0.46 0.43 1</diffuse>\n",
    "  <specular>0.10 0.10 0.09 1</specular>\n",
    "  <emissive>0 0 0 1</emissive>\n",
    "</material>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a84203",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "source": [
    "- Magic Command: Issues and breaking of .sdf/.urdf file can be checked using:\n",
    "syntax:\n",
    "\n",
    "    `xmllint --noout --format </path>`\n",
    "\n",
    "    xmllint --noout --format /home/ah/nav_ws/install/ros2_nav/share/ros2_nav/worlds/ugv_world.sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71099c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "xmllint --noout --format /home/ah/nav_ws/install/ros2_nav/share/ros2_nav/worlds/ugv_world.sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9947f",
   "metadata": {},
   "source": [
    " Add mass and inertial properties for physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a22c8c",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<!-- Add inertial properties for physics-->\n",
    "<model name=\"walkie_talkie_2\">\n",
    "      <pose>4.099660 -4.977570 1.139800 1.452840 -1.361920 -3.091190</pose>\n",
    "      <static>false</static>\n",
    "      <link name=\"body\">\n",
    "        <inertial>\n",
    "          <mass>0.7</mass>\n",
    "          <inertia>\n",
    "            <ixx>0.002</ixx>\n",
    "            <ixy>0</ixy>\n",
    "            <ixz>0</ixz>\n",
    "            <iyy>0.002</iyy>\n",
    "            <iyz>0</iyz>\n",
    "            <izz>0.003</izz>\n",
    "          </inertia>\n",
    "          <pose>0 0 0 0 0 0</pose>\n",
    "        </inertial>\n",
    "\n",
    "        <visual name=\"walkie_talkie_visual\">\n",
    "          <geometry>\n",
    "            <mesh>\n",
    "              <uri>file://models/walkie-talkie/source/walkyTalky 3/walkie_talkie.dae</uri>\n",
    "              <scale>0.0003 0.0003 0.0003</scale>\n",
    "            </mesh>\n",
    "          </geometry>\n",
    "        </visual>\n",
    "\n",
    "        <collision name=\"walkie_talkie_collision\">\n",
    "          <geometry>\n",
    "            <mesh>\n",
    "              <uri>file://models/walkie-talkie/source/walkyTalky 3/walkie_talkie.dae</uri>\n",
    "              <scale>0.0003 0.0003 0.0003</scale>\n",
    "            </mesh>\n",
    "          </geometry>\n",
    "        </collision>\n",
    "        <self_collide>false</self_collide>\n",
    "        <enable_wind>false</enable_wind>\n",
    "        <kinematic>false</kinematic>\n",
    "      </link>\n",
    "    </model>\n",
    "\n",
    "<!-- Add mass so Gazebo physics can simulate fall -->\n",
    "<model name=\"gernade_4\">\n",
    "      <pose>-2.02990 3.872050 0.035452 -1.292830 0.607715 -1.714630</pose>\n",
    "      <static>false</static>\n",
    "      <link name=\"body\">\n",
    "        <!-- Add mass so Gazebo physics can simulate fall -->\n",
    "        <inertial>\n",
    "          <mass>0.2</mass>\n",
    "          <inertia>\n",
    "            <ixx>0.002</ixx>\n",
    "            <iyy>0.002</iyy>\n",
    "            <izz>0.002</izz>\n",
    "          </inertia>\n",
    "        </inertial>\n",
    "        <visual name=\"gun_visual\">\n",
    "          <geometry>\n",
    "            <mesh>\n",
    "              <uri>file://models/gernade.dae</uri>\n",
    "              <scale>1 1 1</scale>\n",
    "            </mesh>\n",
    "          </geometry>\n",
    "        </visual>\n",
    "        <collision name=\"gun_collision\">\n",
    "          <geometry>\n",
    "             <mesh>\n",
    "              <uri>file://models/gernade.dae</uri>\n",
    "              <scale>1 1 1</scale>\n",
    "            </mesh>\n",
    "          </geometry>\n",
    "        </collision>\n",
    "      </link>\n",
    "    </model>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63d2e5",
   "metadata": {},
   "source": [
    "#### Topics for robots position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473c987",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# local\n",
    "\n",
    "ros2 topic echo /odometry/filtered\n",
    "\n",
    "ros2 run tf2_ros tf2_echo map base_link \n",
    "# is not a topic — it’s a command-line tool that queries the /tf topic in real time and prints the transform between map and base_link.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c024f37",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from tf2_ros import Buffer, TransformListener\n",
    "rclpy.init()\n",
    "node = Node('tf_listener')\n",
    "tf_buffer = Buffer(); tf_listener = TransformListener(tf_buffer, node)\n",
    "t = tf_buffer.lookup_transform('map', 'base_link', rclpy.time.Time())  # same as tf2_echo\n",
    "print(t.transform.translation)  # robot position in map frame\n",
    "# better to write a node that prints this or directly include in some node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd7aaf",
   "metadata": {},
   "source": [
    "#### Blender\n",
    "\n",
    "https://snapcraft.io/blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184e3fa",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835bb62",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Fast build and .gitignore file\n",
    "\n",
    "# MacOS system files\n",
    "**/.DS_Store\n",
    "\n",
    "# Virtual Environments\n",
    "**/venv/\n",
    "\n",
    "# Windows system files\n",
    "**/Thumbs.db\n",
    "\n",
    "# VSCode settings\n",
    ".vscode/\n",
    "\n",
    "# Python cache files\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    "\n",
    "# ROS2 workspace build artifacts\n",
    "build/\n",
    "install/\n",
    "log/\n",
    "\n",
    "# Ignore recursively inside any subfolder\n",
    "**/build/\n",
    "**/install/\n",
    "**/log/\n",
    "\n",
    "# CMake generated files\n",
    "CMakeFiles/\n",
    "CMakeCache.txt\n",
    "Makefile\n",
    "*.cmake\n",
    "\n",
    "# Other temporary files\n",
    "*.swp\n",
    "*~\n",
    "\n",
    "\n",
    "# \n",
    "# sudo apt install ros-jazzy-tf-transformations\n",
    "# pip install --upgrade pip setuptools colcon-common-extensions\n",
    "# remove or comment     #tests_require=['pytest'], this line in setup.py of each package\n",
    "# sudo apt install ros-jazzy-interactive-marker-twist-server\n",
    "# git clone https://github.com/MOGI-ROS/m-explore-ros2.git\n",
    "# sudo apt update\n",
    "# sudo apt install python3-pyqt5\n",
    "# sudo apt install ros-jazzy-nav2-msgs ros-jazzy-nav2-bringup\n",
    "# sudo apt install ros-jazzy-nav2-route\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301500cc",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "----\n",
    "Commands for UGV sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd1ba6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd ~\n",
    "source /opt/ros/jazzy/setup.bash\n",
    "cd nav_ws\n",
    "source ~/nav_ws/install/setup.bash\n",
    "colcon build --symlink-install\n",
    "\n",
    "# In terminal 1: Run Gazebo with the UGV world\n",
    "ros2 run ros2_nav spawn_robot.launch.py world:=ugv_world.sdf\n",
    "\n",
    "# new terminal 2: Run teleop to control the robot\n",
    "source /opt/ros/jazzy/setup.bash\n",
    "ros2 run teleop_twist_keyboard teleop_twist_keyboard\n",
    "\n",
    "colcon list # to see all packages in workspace\n",
    "ros2 pkg list | grep vision # to see if vision package is built and available\n",
    "\n",
    "# new terminal 3: vision launch \n",
    "cd ~/nav_ws\n",
    "source /opt/ros/jazzy/setup.bash\n",
    "source ~/nav_ws/install/setup.bash\n",
    "colcon build --symlink-install --packages-select vision\n",
    "ros2 launch vision vision.launch.py\n",
    "\n",
    "# new terminal 4: To view camera frames in window\n",
    "rqt\n",
    "# In rqt, go to Plugins -> Visualization -> Image View, then select the topic /detection/image_3d to view the camera feed.\n",
    "# similar for 2nd image topic (/camera/depth_image), grab the title bar, drag it to right edge of the other image view. When we see blue docking highlight, release to review side by side.\n",
    "\n",
    "# new terminal 5: To capture camera frames for genai_ws tasks\n",
    "cd ~/genai_ws\n",
    "source /opt/ros/jazzy/setup.bash\n",
    "source install/setup.bash\n",
    "colcon build --symlink-install --packages-select pre_processing\n",
    "ros2 launch pre_processing preprocessing.launch.py\n",
    "\n",
    "# new terminal 6: To run the field observer agent\n",
    "cd ~/genai_ws\n",
    "source /opt/ros/jazzy/setup.bash\n",
    "source install/setup.bash\n",
    "source venv/bin/activate\n",
    "\n",
    "cd src/agentc\n",
    "python3 fo_agent.py\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
