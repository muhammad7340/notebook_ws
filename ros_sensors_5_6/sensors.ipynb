{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb37ffd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Config files**:\n",
    "- Usually written in .yaml(yet another markup language) or .json(JavaScript Object Notation) or .xml(Extensible Markup Language) formats, config files are used to store `parameters`, `settings` and `preferences` for software applications.\n",
    "- They let you adjust robot behavior (sensor limits, PID gains, topic names, frame IDs, robot dimensions, map paths, etc.) without recompiling\n",
    "- ROS2 uses them heavily with nodes via the --ros-args --params-file option, so you can tune parameters at runtime.\n",
    "\n",
    "**Launch files**:\n",
    "- Launch files are XML or Python files that allow you to start multiple nodes and set their parameters in a single command.\n",
    "- They are especially useful for complex systems with many nodes and parameters.\n",
    "\n",
    "`In .xml file nested comments arent allowed!`\n",
    "so use   <ignore_block> </ignore_block>   to comment out blocks of code \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a426898",
   "metadata": {},
   "source": [
    "**PID**\n",
    "\n",
    "PID gains are the three tuning parameters of a PID controller:\n",
    "- P (Proportional gain) ‚Üí corrects error based on its size (bigger error ‚Üí stronger correction).\n",
    "- I (Integral gain) ‚Üí eliminates steady-state error by accumulating past error.\n",
    "- D (Derivative gain) ‚Üí predicts future error by reacting to the rate of change, damping oscillations.\n",
    "\n",
    "They are helpful in mobile robotics for precise motion control:\n",
    "- Keeping wheel speeds at the commanded velocity.\n",
    "- Ensuring smooth trajectory tracking.\n",
    "- Stabilizing heading or position when following paths.\n",
    "\n",
    "**`Example`**: In a differential drive robot, PID gains ensure each wheel reaches and maintains the desired RPM despite friction, load, or uneven surfaces.\n",
    "\n",
    "**Transient vs Steady State**\n",
    "\n",
    "- Before steady state ‚Üí when a command is first given (e.g., ‚Äúwheel at 1 m/s‚Äù), the robot is in a transient state. The speed ramps up, may overshoot or oscillate depending on gains.\n",
    "- Steady state ‚Üí after some time, the wheel settles close to the target speed and stays there. The error becomes very small (ideally zero).\n",
    "- After steady state changes ‚Üí if conditions shift (robot climbs a slope, carries extra weight, battery voltage drops), the system temporarily leaves steady state, and the controller must react to reach the new steady state again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc75f31",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556ca09",
   "metadata": {},
   "source": [
    "**`Role of robot_state_publisher`**\n",
    "- It publishes the TF tree of your robot ‚Üí how all links/joints move relative to each other.\n",
    "- Input:\n",
    "   - Your URDF (from robot_description param).\n",
    "   - Joint states (/joint_states).\n",
    "- Output:\n",
    "   - TF frames (/tf and /tf_static).\n",
    "- Without it:\n",
    "- RViz can‚Äôt visualize robot links correctly.\n",
    "- Other ROS nodes can‚Äôt know where sensors/arms/wheels are.\n",
    "- Adding the camera URDF update was essential so the camera frame appeared in TF tree.\n",
    "\n",
    "\n",
    "**`Role of ros_gz_bridge`**\n",
    "- Gazebo (Ignition/GZ Sim) speaks Gazebo Transport messages(gz.msgs.*).\n",
    "- ROS2 speaks ROS2 messages (sensor_msgs, geometry_msgs).\n",
    "- They are different protocols.\n",
    "- ros_gz_bridge maps one to the other by parameter gz_bridge_node of ros_gz_bridge package.\n",
    "- Example:\n",
    "   - /camera/image in Gazebo ‚Üí bridged ‚Üí /camera/image in ROS2.\n",
    "   - /cmd_vel in ROS2 ‚Üí bridged ‚Üí /cmd_vel in Gazebo (to control robot).\n",
    "- Without it:\n",
    "   - Your sensors (camera, lidar, IMU) would only exist inside Gazebo ‚Üí ROS2 nodes (like RViz or SLAM) couldn‚Äôt use them.\n",
    "   - Your control commands in ROS2 couldn‚Äôt reach the robot in simulation.\n",
    "   - [link to add more topics:](https://github.com/gazebosim/ros_gz/tree/ros2/ros_gz_bridge)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf4659",
   "metadata": {},
   "source": [
    "**Sensors**\n",
    "\n",
    "Sensors are devices that measure physical properties (distance, light, sound, temperature, etc.) and convert them into signals that can be read by a robot's computer.\n",
    "- Common sensors in robotics include:\n",
    "  - LIDAR (Light Detection and Ranging) for mapping and obstacle detection.\n",
    "  - Cameras for vision and object recognition.\n",
    "  - IMUs (Inertial Measurement Units) for orientation and motion tracking.\n",
    "  - GPS for outdoor localization.\n",
    "  - Ultrasonic sensors for distance measurement.\n",
    "  - Encoders for measuring wheel rotation and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe84b8",
   "metadata": {},
   "source": [
    "If it's unclear what kind of properties are available for which sensors, the official SDF [reference manual](http://sdformat.org/spec?ver=1.6&elem=sensor#camera_depth_camera) and Documentation for [SDF Syntax with code](http://sdformat.org/tutorials?) is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1e4b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918d33e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Camera \n",
    "\n",
    "- Camera is represented by a small cube. Represented by a fixed joint attched to base link. \n",
    "- Another joint names `camera_link_optical` is connected to `camera_link` which is additional link &joint to`solve conflict` between `2 different coordinates systems`\n",
    "    - By default, URDF uses `right-handed coordinate system` with X forward, Y left, and Z up.\n",
    "    - However, many ROS drivers and vision processing pipelines expect a camera‚Äôs optical axis to be aligned with Z forward, X to the right, and Y down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bd8b9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ros2 param set /robot_state_publisher robot_description \"$(xacro $(ros2 pkg prefix gazebo_sensors)/share/gazebo_sensors/urdf/mob_bot_mecanum.urdf)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45d59e",
   "metadata": {},
   "source": [
    "- `ros2 param set` ‚Üí tells a running node to update one of its parameters.\n",
    "- `/robot_state_publisher` ‚Üí the node we target.\n",
    "- `robot_description` ‚Üí the key parameter holding the URDF string.\n",
    "- `\"$(xacro ...)\"` ‚Üí expands the URDF (runs xacro to generate plain XML).\n",
    "- That whole URDF is then injected directly into the running robot_state_publisher.\n",
    "\n",
    "Why we do this?\n",
    "- To reload the robot model on the fly without restarting nodes.\n",
    "- Helps when debugging URDF/Xacro edits (like adding a sensor, changing joint names, fixing links).\n",
    "\n",
    "Why here in Camera?:\n",
    "- We added a camera link/joint to your robot URDF (red cube visual).\n",
    "- Gazebo spawned it, so visually you saw the cube.\n",
    "- But ROS2 did not know about the new link/joint because the robot_state_publisher was still running with the old URDF.\n",
    "- Result ‚Üí rqt_graph/rqt_topic showed no camera topics.\n",
    "\n",
    "Camera Image Topic\n",
    "- Open rqt and select plugins->topic->Monitor and select /camera/image topic.\n",
    "- Then go again to Plugins->Visualization->Image View to see the camera image.\n",
    "- Or run directly in terminal \n",
    "```bash\n",
    "ros2 run rqt_image_view rqt_image_view\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb5f0f",
   "metadata": {},
   "source": [
    "[Next Issue]\n",
    "Its consuming 6-20 MB bandwidth which is unacceptible for a mobile bot.There is a dedicated `image_bridge` node in the `ros_gz_image` package which can compress mages and make it suitable for mobile robots with KBandwidth constraints.B\n",
    "- Install to use this: `sudo apt install ros-jazzy-compressed-image-transport`\n",
    "- Add the node in launch file\n",
    "\n",
    "[New Issue]\n",
    "RViz always expect the image and the camera_info topics with the same prefix which works well for: `/camera/image` ‚Üí `/camera/camera_info` But doesn't work for: `/camera/image/compressed` ‚Üí `/camera/image/camera_info`. remaping topic still wont resolve this issue as uncompressed image wont work in RViz so use another tool:\n",
    "- the `relay node` from the `topic_tools package`.\n",
    "- Relay node to republish `/camera/camera_info` to `/camera/image/camera_info` add node in launch file and dont forgot to add in launchDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f91c47",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# do changes in urdf, and dont relaunch again and again, just run this  and things get updated!\n",
    "\n",
    "ros2 param set /robot_state_publisher robot_description \"$(cat ~/gz_sensors_ws/gazebo_sensors/urdf/mob_bot_mecanum.urdf)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba46d28",
   "metadata": {},
   "source": [
    "`rqt_reconfigure` is generally used for dynamically reconfiguring parameters of ROS nodes, including those related to sensors. This can be particularly useful for adjusting camera properties like image resolution, update rate, etc. on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01bb26",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# But how do we know what is the name of the parameter \n",
    "# and what other settings do we can change? \n",
    "# To see that we will use the rqt_reconfigure node.\n",
    "# launch executables first!, Then start rqt_reconfigure:\n",
    "ros2 run rqt_reconfigure rqt_reconfigure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4cd11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40324b",
   "metadata": {},
   "source": [
    "RQT Graph/Topic\n",
    "\n",
    "![rqt](../assets/rqt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e333c17",
   "metadata": {},
   "source": [
    "Explanation:  Gazebo ‚Üí Bridge ‚Üí ROS2 Flow\n",
    "\n",
    "1. Simulation side (Gazebo)\n",
    "\n",
    "- **Gazebo world + robot** ‚Üí runs the physics and sensors.  \n",
    "- **Files**:\n",
    "  - `.gazebo` ‚Üí config for plugins (camera, lidar, joints).\n",
    "  - `.urdf` / `.xacro` ‚Üí robot description (links, joints, sensors).\n",
    "- **Role**: Together, these tell Gazebo what the robot is and how it behaves.  \n",
    "Example: your camera sensor in `.gazebo` file produces `gz.msgs.Image`, not directly a ROS topic.\n",
    "\n",
    "\n",
    "2. ros_gz_bridge (the translator node)\n",
    "\n",
    "- Node: `/ros_gz_bridge`\n",
    "- **Function**: Bridges Gazebo Transport ‚Üî ROS 2 topics.\n",
    "- **Examples**:\n",
    "  - `/camera/image` ‚Üí `sensor_msgs/msg/Image`\n",
    "  - `/odom` ‚Üí `nav_msgs/msg/Odometry`\n",
    "  - `/joint_states` ‚Üí `sensor_msgs/msg/JointState`\n",
    "  - `/tf` ‚Üí `tf2_msgs/msg/TFMessage`\n",
    "\n",
    "This makes Gazebo data visible to ROS 2 nodes.\n",
    "\n",
    "\n",
    "3. ROS2 side (nodes + topics)\n",
    "\n",
    "- **/robot_state_publisher**\n",
    "  - Reads `robot_description` (URDF/Xacro XML) + `/joint_states`.\n",
    "  - Publishes **TF tree** (frames: `base_link ‚Üí camera_link`).\n",
    "  - Needed for RViz + Nav2.\n",
    "\n",
    "- **/trajectory_server & /trajectory**\n",
    "  - Custom node in your package.\n",
    "  - Consumes trajectory commands and interacts with robot via TF + odometry.\n",
    "\n",
    "- **/tf + /robot_description**\n",
    "  - `/robot_description` = URDF pushed as parameter (from launch file).\n",
    "  - `/tf` = live coordinate transforms.\n",
    "\n",
    "- **/cmd_vel**\n",
    "  - Velocity commands from teleop/navigation/trajectory server.\n",
    "  - Bridged ‚Üí robot moves in Gazebo.\n",
    "\n",
    "- **/camera/image + /camera/camera_info**\n",
    "  - Data from Gazebo camera plugin.\n",
    "  - Bridged ‚Üí ROS topics.\n",
    "  - Usable in RViz or perception nodes.\n",
    "\n",
    "4. Where each file fits\n",
    "\n",
    "- **URDF/Xacro** (`robot.urdf.xacro`)\n",
    "  - Defines robot structure, joints, sensors.\n",
    "  - Consumed by `robot_state_publisher`.\n",
    "\n",
    "- **Gazebo files** (`.gazebo` / `.sdf`)\n",
    "  - Define simulation-specific plugins (camera noise, lidar range).\n",
    "  - Consumed by **Gazebo**, not ROS directly.\n",
    "\n",
    "- **Launch files** (`.launch.py`)\n",
    "  - Start everything:\n",
    "    - Spawn robot in Gazebo.\n",
    "    - Start `ros_gz_bridge`.\n",
    "    - Start `robot_state_publisher`.\n",
    "    - Push `robot_description` parameter.\n",
    "\n",
    "5. Pipeline Flow\n",
    "\n",
    "  - **Gazebo** runs robot with physics + sensors (URDF + plugins).\n",
    "  - Sensors + states ‚Üí produced in Gazebo (raw Gazebo msgs).\n",
    "  - **ros_gz_bridge** translates ‚Üí ROS 2 topics.\n",
    "  - **robot_state_publisher** builds TF tree from URDF + joint states.\n",
    "  - ROS 2 ecosystem (RViz, SLAM, Nav2, trajectory servers) uses `/odom`, `/camera/image`, `/tf`, `/cmd_vel`.\n",
    "  - ROS nodes publishing `/cmd_vel` ‚Üí bridged back to Gazebo ‚Üí robot moves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f702bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91115600",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "  <!--Camera model in .gazebo file-->\n",
    "  <gazebo reference=\"camera_link\"><!--refer to link we defined in URDF-->\n",
    "    <sensor name=\"camera\" type=\"camera\">\n",
    "      <camera> <!--define below camera related settings-->\n",
    "        <horizontal_fov>1.3962634</horizontal_fov><!--camera's field of view here in radians-->\n",
    "        <image>\n",
    "          <width>640</width>\n",
    "          <height>480</height>\n",
    "          <format>R8G8B8</format><!--RGB 8bit pixel format-->\n",
    "        </image>\n",
    "        <clip><!--min-max distance camera can see-->\n",
    "          <near>0.1</near><!--0.1meter-->\n",
    "          <far>15</far>\n",
    "        </clip>\n",
    "        <noise><!--model real world imperfections-->\n",
    "          <type>gaussian</type>\n",
    "          <!-- Noise is sampled independently per pixel on each frame.\n",
    "               That pixel's noise value is added to each of its color\n",
    "               channels, which at that point lie in the range [0,1]. -->\n",
    "          <mean>0.0</mean>\n",
    "          <stddev>0.007</stddev>\n",
    "        </noise>\n",
    "        <optical_frame_id>camera_link_optical</optical_frame_id>\n",
    "        <camera_info_topic>camera/camera_info</camera_info_topic><!--topic publishing intrinsic/extrinsic calibration info-->\n",
    "        <!--tools like rviz requires a camera_info topic that describes the physical properties of the camera. The topic's name must match camera's topic (in this case both are camera/...camera topic defined below-->\n",
    "      </camera>\n",
    "      <always_on>1</always_on>\n",
    "      <update_rate>20</update_rate><!--Sensor data publisher 20 times per sec (Hz)-->\n",
    "      <visualize>true</visualize><!--sensor output view in gazebo enabled-->\n",
    "      <topic>camera/image</topic><!--ROS/Gazebo topic where camera images are published-->\n",
    "      <!--from SDformat.org-->\n",
    "      <!--<save>\n",
    "        <enabled></enabled>\n",
    "        <path></path>\n",
    "      </save>-->\n",
    "    </sensor>\n",
    "  </gazebo>\n",
    "\n",
    "\n",
    "  <!--wide angle camera model in .gazebo file-->\n",
    "  <!--Wide angle Camera-->\n",
    "  <!--\n",
    "  <gazebo reference=\"camera_lens_link\">\n",
    "    <sensor name=\"wideangle_camera\" type=\"wideanglecamera\">\n",
    "      <camera>\n",
    "        <horizontal_fov>3.14</horizontal_fov>\n",
    "        <image>\n",
    "          <width>640</width>\n",
    "          <height>480</height>\n",
    "        </image>\n",
    "        <clip>\n",
    "          <near>0.1</near>\n",
    "          <far>15</far>\n",
    "        </clip>\n",
    "        <optical_frame_id>camera_link_optical</optical_frame_id>\n",
    "        <camera_info_topic>camera/camera_info</camera_info_topic>\n",
    "      </camera>\n",
    "      <always_on>1</always_on>\n",
    "      <update_rate>20</update_rate>\n",
    "      <topic>camera/image</topic>\n",
    "      <gz_frame_id>camera_link</gz_frame_id>\n",
    "    </sensor>\n",
    "  </gazebo>\n",
    "  -->\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51facb7a",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<!--Old Camera Working code-->\n",
    " <!-- STEP 7 - Camera -->\n",
    "  <joint type=\"fixed\" name=\"camera_joint\">\n",
    "    <origin xyz=\"0.225 0 0.075\" rpy=\"0 0 0\"/>\n",
    "    <child link=\"camera_link\"/>\n",
    "    <parent link=\"base_link\"/>\n",
    "    <axis xyz=\"0 1 0\" />\n",
    "  </joint>\n",
    "\n",
    "  <link name='camera_link'>\n",
    "    <pose>0 0 0 0 0 0</pose>\n",
    "    <inertial>\n",
    "      <mass value=\"0.1\"/>\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <inertia\n",
    "          ixx=\"1e-6\" ixy=\"0\" ixz=\"0\"\n",
    "          iyy=\"1e-6\" iyz=\"0\"\n",
    "          izz=\"1e-6\"\n",
    "      />\n",
    "    </inertial>\n",
    "\n",
    "    <collision name='collision'>\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/> \n",
    "      <geometry>\n",
    "        <box size=\".04 .06 .04\"/>\n",
    "      </geometry>\n",
    "    </collision>\n",
    "\n",
    "    <visual name='camera_link_visual'>\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <geometry>\n",
    "        <box size=\".04 .06 .04\"/>\n",
    "      </geometry>\n",
    "      <material name=\"grey\"/>\n",
    "    </visual>\n",
    "\n",
    "    <visual name='camera_link_visual_lens'>\n",
    "      <origin xyz=\"0.032 0 0\" rpy=\"0 1.57 0\"/>\n",
    "      <geometry>\n",
    "        <cylinder radius=\"0.015\" length=\"0.027\"/>\n",
    "      </geometry>\n",
    "      <material name=\"black\"/>\n",
    "    </visual>\n",
    "  </link>\n",
    "\n",
    "  <joint type=\"fixed\" name=\"camera_optical_joint\">\n",
    "    <origin xyz=\"0.25 0 0\" rpy=\"-1.5707 0 -1.5707\"/>\n",
    "    <child link=\"camera_link_optical\"/>\n",
    "    <parent link=\"camera_link\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_link_optical\">\n",
    "  </link>\n",
    "\n",
    "\n",
    "  <!-- STEP 7 - Camera -->\n",
    "  <!--new-->\n",
    "  <joint type=\"fixed\" name=\"camera_joint\">\n",
    "    <origin xyz=\"0.220 0 0.075\" rpy=\"0 0 0\"/>\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child  link=\"camera_link\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_link\">\n",
    "    <inertial>\n",
    "      <mass value=\"0.1\"/>\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <inertia ixx=\"1e-6\" ixy=\"0\" ixz=\"0\" iyy=\"1e-6\" iyz=\"0\" izz=\"1e-6\"/>\n",
    "    </inertial>\n",
    "\n",
    "    <!-- body box that is only decorative and collision -->\n",
    "    <collision name=\"camera_body_collision\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <geometry><box size=\"0.04 0.03 0.03\"/></geometry>\n",
    "    </collision>\n",
    "\n",
    "    <visual name=\"camera_body_visual\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <geometry><box size=\"0.04 0.06 0.04\"/></geometry>\n",
    "      <material name=\"grey\"/>\n",
    "    </visual>\n",
    "  </link>\n",
    "\n",
    "  <!-- separate lens link positioned forward of the box -->\n",
    "  <joint type=\"fixed\" name=\"camera_lens_joint\">\n",
    "    <parent link=\"camera_link\"/>\n",
    "    <!-- put lens where you want the optical center to be -->\n",
    "    <origin xyz=\"0.025 0 0\" rpy=\"0 0 0\"/>\n",
    "    <child  link=\"camera_lens_link\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_lens_link\">\n",
    "    <!-- lens visual placed at this link origin -->\n",
    "    <visual name=\"camera_lens_visual\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 1.5707 0\"/>\n",
    "      <geometry>\n",
    "        <cylinder radius=\"0.015\" length=\"0.03\"/>\n",
    "      </geometry>\n",
    "      <material name=\"black\"/>\n",
    "    </visual>\n",
    "  </link>\n",
    "\n",
    "  <!-- optical frame now coincides with lens link origin -->\n",
    "  <joint type=\"fixed\" name=\"camera_optical_joint\">\n",
    "    <parent link=\"camera_lens_link\"/>\n",
    "    <child  link=\"camera_link_optical\"/>\n",
    "    <!-- optical joint placed at lens center so sensor uses lens link as reference -->\n",
    "    <origin xyz=\"0 0 0\" rpy=\"-1.5707 0 -1.5707\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_link_optical\"></link>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d0064",
   "metadata": {},
   "source": [
    "Solve header issue of wide angle camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a7926",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ros2 run bme_gazebo_sensors_py image_republisher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfeb6d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "IMU\n",
    "\n",
    "An Inertial Measurement Unit (IMU) typically consists of:  \n",
    "- a 3-axis **accelerometer** \n",
    "    - measure `linear acceleration (X,Y,Z)` (rate of change of velocity along x,y,z axis)\n",
    "    - detect gravity, bumps,vibrations. \n",
    "    - Also used to estimate orientation\n",
    "- a 3-axis **gyroscope**\n",
    "    - Measures `angular velocity(roll,pitch,yaw)` around X, Y, Z axes.\n",
    "    - These correspond to roll, pitch, yaw rates (how fast you rotate, or turn)\n",
    "- a 3-axis **magnetometer**\n",
    "    - Measures the `Earth‚Äôs magnetic field vector (yaw)`.\n",
    "    - Used as a digital compass ‚Üí helps estimate absolute heading (yaw/orientation).\n",
    "    - Measures facing North vs facing East, etc.\n",
    "\n",
    "\n",
    "    IMU measures linear acceleration, angular velocity, and possibly magnetic heading (orientation). It's important to remember that it's `not possible` to `measure uniform motion` with an IMU where the `velocity is constant` (`acceleration is zero`) and there is `no change in the orientation`. Therefore we cannot replace the odometry of the robot with an IMU but with the right technique we can combine these two into a more precise measurement unit.\n",
    "\n",
    "*** Always add `topics` of plugins defined in .gazebo file to the bridge node in launch file to make them available in ROS2.\n",
    "\n",
    "---\n",
    "\n",
    "**Odometry**:\n",
    "It's estimating a robot‚Äôs position (x, y) and orientation (Œ∏/yaw) over time by integrating motion information from its sensors.\n",
    "For a differential drive robot (two wheels) example:\n",
    "- Each wheel has an encoder that counts how many ticks it rotated.\n",
    "- From encoder ticks ‚Üí we get distance traveled by each wheel.\n",
    "\n",
    "Odometry Sources\n",
    "- Wheel encoders (most common) ‚Üí measure wheel rotations.\n",
    "- Visual odometry (camera-based, e.g. ORB-SLAM) ‚Üí track visual features.\n",
    "- Lidar odometry (scan matching).\n",
    "\n",
    "**Drift**\n",
    "- Drift in wheel encoders = the cumulative error in odometry as the robot moves.\n",
    "- It happens because encoders only measure wheel rotations, not actual ground motion.\n",
    "- If wheels slip (e.g., on smooth floor, turning, or uneven ground), ticks still increase but the robot hasn‚Äôt moved that distance.\n",
    "- Over time, these small errors add up ‚Üí robot‚Äôs `estimated position` `drifts away` from its `true position`\n",
    "\n",
    "**Need of `Sensor Fusion`?**\n",
    "\n",
    "Reason: IMU's cant measure constant velocity motion (zero acceleration) and odometry drifts over time due to wheel slip, uneven terrain, etc.\n",
    "- We combine IMU + odometry using sensor fusion (e.g., an Extended Kalman Filter, EKF).\n",
    "- Odometry gives long-term position tracking but drifts with wheel slip.\n",
    "- IMU gives short-term motion detection but can‚Äôt see constant motion.\n",
    "- Together ‚Üí more precise state estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286974a",
   "metadata": {},
   "source": [
    "---\n",
    "Plugins in World Files For simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3264b",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "# add in world.sdf files:\n",
    "\n",
    "    <plugin name='gz::sim::systems::Physics' filename='gz-sim-physics-system'/>\n",
    "    <plugin name='gz::sim::systems::UserCommands' filename='gz-sim-user-commands-system'/>\n",
    "    <plugin name='gz::sim::systems::SceneBroadcaster' filename='gz-sim-scene-broadcaster-system'/>\n",
    "    <plugin name='gz::sim::systems::Contact' filename='gz-sim-contact-system'/>\n",
    "    <plugin\n",
    "      filename=\"gz-sim-sensors-system\"\n",
    "      name=\"gz::sim::systems::Sensors\">\n",
    "      <render_engine>ogre2</render_engine>\n",
    "    </plugin>\n",
    "    <plugin\n",
    "      filename=\"gz-sim-imu-system\"\n",
    "      name=\"gz::sim::systems::Imu\">\n",
    "    </plugin>\n",
    "    <plugin\n",
    "      filename=\"gz-sim-navsat-system\"\n",
    "      name=\"gz::sim::systems::NavSat\">\n",
    "    </plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b011719",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. **Linear Systems**\n",
    "\n",
    "A linear system means its equations obey `superposition`:\n",
    "- If input A ‚Üí output X, then input B ‚Üí output Y,  \n",
    "- Then input (A + B) ‚Üí output (X + Y). (`Additivity` Property)\n",
    "- If input A ‚Üí output X, then input (k*A) ‚Üí output (k*X). (`Homogeneity` Property)\n",
    "- Example:  x(k+1) = x(k) + v * Œît  \n",
    "> Velocity v just scales linearly with time.  \n",
    "> Graph: `straight line relation` (no curves, no trig).  \n",
    "> Linear Kalman Filter works perfectly here.  \n",
    "\n",
    "2. **Non-linear Systems**\n",
    "\n",
    "A non-linear system breaks superposition: doubling the input does not double the output.  \n",
    "Equations involve `sin`, `cos`, `products of states`, `division` etc.  \n",
    "- Example: Robot orientation update:  \n",
    "    - x(k+1) = x(k) + v * cos(Œ∏) * Œît  \n",
    "    - y(k+1) = y(k) + v * sin(Œ∏) * Œît  \n",
    ">Here motion depends on cos(Œ∏), sin(Œ∏) ‚Üí non-linear functions.  \n",
    ">If robot turns by 90¬∞, cos and sin change abruptly.  \n",
    ">A linear model can‚Äôt handle this curvature.  \n",
    "\n",
    "3. Why **Linear KF fails**\n",
    "\n",
    "Linear KF assumes system dynamics can be written as:  \n",
    "- x(k+1) = A x(k) + B u(k) + w(k)  (Motion model)\n",
    "- z(k) = H x(k) + v(k)             (Measurement model)\n",
    "where A, B, H are matrices (linear transforms).  \n",
    "- But real robot motion (turning, IMU readings, GPS with Earth curvature) ‚Üí involves sin, cos, quaternions.If we force-fit them into a linear model:  \n",
    "    - Predictions diverge (robot thinks it‚Äôs straight when it‚Äôs curving).  \n",
    "    - Measurements mismatch badly.  \n",
    "\n",
    "4. **Extended Kalman Filter (EKF)**\n",
    "\n",
    "EKF is a fix for this: it can work with non-linear dynamics and measurements. `Key idea: Local linearization` \n",
    "- Take your non-linear equations:  \n",
    "    - x(k+1) = f(x(k), u(k)) + w(k)  (Motion model) \n",
    "    - z(k) = h(x(k)) + v(k)          (Measurement model)\n",
    "\n",
    "where f and h are non-linear (sin, cos, etc).  \n",
    "\n",
    "- EKF approximates them as linear near the current estimate using Jacobian matrices (derivatives).  \n",
    "- So instead of globally linear, EKF says:  \n",
    "   - ‚ÄúOkay, around this small region, motion is almost linear.‚Äù  \n",
    "\n",
    "5. `***EKF Steps***`\n",
    "- `Prediction` (non-linear motion model): Use f(x,u) (with cos, sin, etc.) to predict where robot should be.  \n",
    "- `Linearize` (Jacobian): Compute derivatives of f and h at current state ‚Üí gives you local linear models.  \n",
    "- `Update` (non-linear measurement model): Compare predicted measurement (from IMU, GPS, encoders) with actual one. Update estimate with corrected covariance.  \n",
    "\n",
    "6. Example\n",
    "- Suppose robot drives forward with velocity v and orientation Œ∏.  Linear KF would say:  \n",
    "    - x(k+1) = x(k) + v * Œît   (always in a straight line).  \n",
    "- Real robot (non-linear):  \n",
    "    - x(k+1) = x(k) + v * cos(Œ∏) * Œît  \n",
    "    - y(k+1) = y(k) + v * sin(Œ∏) * Œît  \n",
    "- If Œ∏ = 90¬∞ ‚Üí robot moves upwards, not straight. KF would fail here. EKF handles it by linearizing cos(Œ∏), sin(Œ∏) locally ‚Üí so update works.  \n",
    "\n",
    "---\n",
    "\n",
    "**EKF**  (State Estimation using Non linear Motion model, Global nonlinear becomes locally linear using Jacoboian, Update nonlinear model using predicted vs actual measurement from sensors)\n",
    "\n",
    "An Extended Kalman Filter (EKF) algorithm is used to estimate vehicle `position`, `velocity` and `angular orientation` based on rate gyroscopes, accelerometer, compass, GPS, airspeed and barometric pressure measurements.\n",
    "\n",
    "- The `advantage` of the EKF over the simpler complementary filter algorithms (i.e. ‚ÄúInertial Nav‚Äù or DCM), is that by fusing all available measurements it is better able to `reject measurements with significant errors`\n",
    "\n",
    "Background:\n",
    "\n",
    "Unlike the standard Kalman Filter, which is designed for linear systems, the EKF can handle `non-linear models`. Real-world systems, such as the movement of a car or a person, rarely follow perfectly linear paths. The EKF addresses this by using a mathematical technique called `linearization`. At each time step, it` approximates the non-linear system` with a `linear one` around the `current state estimate` using `*Jacobian`. This allows it to apply the same `predict-and-update cycle` as the standard Kalman Filter.\n",
    "\n",
    "The cycle works as follows:\n",
    "- `Prediction`: The EKF predicts the system's next state based on its current estimate and a motion model. This prediction inherently includes some uncertainty.\n",
    "- `Update`: The filter then incorporates a new measurement from a sensor (like a camera or GPS). It compares the actual measurement to the predicted measurement to calculate a correction, which is then used to update and refine the state estimate.\n",
    "\n",
    "[EKF explained simply link](https://simondlevy.github.io/ekf-tutorial/) (Mathematics + Code + Examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5c8da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "EKF (Must be understood for interview) **\n",
    "\n",
    "1. General EKF Equations\n",
    "\n",
    "`Motion (Process) Model:`\n",
    "- x(k+1) = f(x(k), u(k)) + w(k)\n",
    "\n",
    "    - x(k): State vector at time step k (what we want to estimate, e.g. robot pose [x, y, Œ∏]).\n",
    "    - u(k): Control input at time step k (e.g. wheel velocities, commanded velocity).\n",
    "    - f(‚ãÖ): Nonlinear function describing how the state evolves (robot motion equations).\n",
    "    - w(k): Process noise (uncertainty in motion, e.g. wheel slip).\n",
    "\n",
    "`Measurement (Observation) Model`:\n",
    "- z(k) = h(x(k)) + v(k)\n",
    "\n",
    "    - z(k): Measurement at time step k (e.g. GPS coordinates, range to landmark).\n",
    "    - h(‚ãÖ): Nonlinear function mapping state ‚Üí sensor space.\n",
    "    - v(k): Measurement noise (sensor uncertainty).\n",
    "\n",
    "2. Example Problem ‚Äî Mobile Robot with GPS\n",
    "\n",
    "- `Robot State`:\n",
    "    - x(k) = [x, y, Œ∏]\n",
    "- `Control Input`: forward velocity `v`, angular velocity `œâ`.\n",
    "- Motion Model (differential drive kinematics):\n",
    "- f(x(k), u(k)) =\n",
    "    [\n",
    "    x + v cos(Œ∏) Œît ; \n",
    "    y + v sin(Œ∏) Œît ; \n",
    "    Œ∏ + œâ Œît\n",
    "    ]\n",
    "- Measurement Model (GPS):\n",
    "    - GPS directly measures position (not orientation):\n",
    "        - h(x(k)) = [x, y]\n",
    "\n",
    "\n",
    "3. Numerical Example\n",
    "\n",
    "- Initial state estimate:\n",
    "    - x0 = [2, 3, 45¬∞]; (robot at (2,3) facing NE).\n",
    "\n",
    "- Control input:\n",
    "    - v = 1.0 m/s (forward)\n",
    "    - œâ = 0 rad/s (straight)\n",
    "    - Œît = 1 s\n",
    "- Process Model:\n",
    "   - x1 =\n",
    "    [\n",
    "    2 + 1‚ãÖcos(45¬∞)‚ãÖ1 ;\n",
    "    3 + 1‚ãÖsin(45¬∞)‚ãÖ1 ;\n",
    "    45¬∞ + 0 ;\n",
    "    ]\n",
    "    =\n",
    "    [2.707, 3.707, 45¬∞]. So the predicted state is (2.71, 3.71, 45¬∞).\n",
    "\n",
    "- Measurement: GPS reports\n",
    "    - z1 = [2.9, 3.6]\n",
    "- Measurement Model:\n",
    "    - h(x1) = [x, y] = [2.707, 3.707]\n",
    "- Innovation (measurement error):\n",
    "    - z ‚àí h(x) = [2.9, 3.6] ‚àí [2.707, 3.707] = [0.193, -0.107]. So GPS says ‚Äúyou‚Äôre 0.193 m more east, 0.107 m more south than your motion model predicted.‚Äù\n",
    "\n",
    "\n",
    "4. What EKF Does With This\n",
    "\n",
    "- Prediction Step:\n",
    "    - Uses f(x,u) to move state forward.\n",
    "    - Uses Jacobian F = ‚àÇf/‚àÇx to propagate uncertainty.\n",
    "- Correction Step:\n",
    "    - Compares predicted measurement h(x) with real sensor reading z.\n",
    "    - Uses Jacobian H = ‚àÇh/‚àÇx to figure out how measurement error relates to state error.\n",
    "    - Updates state estimate:\n",
    "        - x(new) = x(pred) + K(z ‚àí h(x(pred))). where K is the Kalman gain (balance between trusting prediction vs measurement).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3d11b",
   "metadata": {},
   "source": [
    "![alt text](assets/k1.png)\n",
    "<img src=\"assets/k2.png\" alt=\"alt text\" width=\"400\" height=\"630\">\n",
    "![alt text](assets/k3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ecadc2",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "source": [
    "Role of Jacobians in EKF (with the same GPS robot example)\n",
    "\n",
    "1. Why Jacobians are needed\n",
    "- Our motion model is nonlinear:\n",
    "    - f(x,u) = [\n",
    "    x + v cos(Œ∏) Œît;\n",
    "    y + v sin(Œ∏) Œît;\n",
    "    Œ∏ + œâ Œît;\n",
    "    ]\n",
    "- The terms cos(Œ∏) and sin(Œ∏) make it nonlinear.\n",
    "- Linear Kalman Filter requires state updates of the form A x + B u (purely linear).\n",
    "- Solution in EKF: locally approximate these nonlinear equations with a linear model around the current estimate. \n",
    "- This is done using Jacobians (matrices of partial derivatives).\n",
    "\n",
    "2. Jacobian of Motion Model\n",
    "- State: x = [x, y, Œ∏]\n",
    "- Control: u = [v, œâ]\n",
    "- Motion update:\n",
    "   - x' = x + v cos(Œ∏) Œît\n",
    "   - y' = y + v sin(Œ∏) Œît\n",
    "   - Œ∏' = Œ∏ + œâ Œît\n",
    "\n",
    "- Jacobian F = ‚àÇf/‚àÇx.  Take partial derivatives of each new state wrt old state [x, y, Œ∏].\n",
    "\n",
    "    - ‚àÇx'/‚àÇx = 1\n",
    "    - ‚àÇx'/‚àÇy = 0\n",
    "    - ‚àÇx'/‚àÇŒ∏ = -v sin(Œ∏) Œît\n",
    "    .....\n",
    "    - ‚àÇy'/‚àÇx = 0\n",
    "    - ‚àÇy'/‚àÇy = 1\n",
    "    - ‚àÇy'/‚àÇŒ∏ = v cos(Œ∏) Œît\n",
    "    .....\n",
    "    - ‚àÇŒ∏'/‚àÇx = 0\n",
    "    - ‚àÇŒ∏'/‚àÇy = 0\n",
    "    - ‚àÇŒ∏'/‚àÇŒ∏ = 1\n",
    "    .....\n",
    "- So\n",
    "    - F =\n",
    "    [ 1   0   -v sin(Œ∏) Œît\n",
    "    0   1    v cos(Œ∏) Œît\n",
    "    0   0    1 ]\n",
    "- This tells how small changes in x,y,Œ∏ affect the next state near current Œ∏.\n",
    "\n",
    "3. Jacobian of Measurement Model\n",
    "- Measurement model: h(x) = [x, y]\n",
    "- We only measure position.\n",
    "\n",
    "    - ‚àÇh/‚àÇx = 1   0   0;\n",
    "              0   1   0. So H =[1 0 0;\n",
    "          0 1 0]\n",
    "\n",
    "4. Numerical Example at Œ∏ = 45¬∞\n",
    "- Let v=1, Œît=1.\n",
    "\n",
    "    - F =\n",
    "    [1   0   -1‚ãÖsin(45¬∞)‚ãÖ1;\n",
    "    0   1    1‚ãÖcos(45¬∞)‚ãÖ1;\n",
    "    0   0    1]\n",
    "    = \n",
    "    [1   0   -0.707;\n",
    "    0   1    0.707;\n",
    "    0   0    1]\n",
    "\n",
    "- This means:\n",
    "    - A small error in Œ∏ produces about -0.707 m change in x and +0.707 m change in y in the next step.\n",
    "    - Exactly what you'd expect: if you‚Äôre slightly wrong about Œ∏, your x/y prediction shifts diagonally.\n",
    "\n",
    "5. Why this works\n",
    "Globally, motion is nonlinear because cos(Œ∏), sin(Œ∏) curve.\n",
    "But around Œ∏=45¬∞, we linearize using the Jacobian ‚Üí straight-line approximation.\n",
    "Next EKF step: use this local linear form to update covariance and combine measurements.\n",
    "At each step, EKF recomputes Jacobians ‚Üí always linearizing around the latest pose.\n",
    "This converts global nonlinear dynamics into locally linear updates that the Kalman filter machinery can handle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e1e3a",
   "metadata": {},
   "source": [
    "![alt text](assets/k4.png)\n",
    "<img src=\"assets/k5.png\" alt=\"alt text\" width=\"560\" height=\"665\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0982586",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537789c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Robot localization package in ROS\n",
    "sudo apt install ros-jazzy-robot-localization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668dfbf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "1. Role of TF in ROS  \n",
    "‚Ä¢ TF keeps track of coordinate frames (map, odom, base_link, camera_link, etc).  \n",
    "‚Ä¢ A transform is \"frame A relative to frame B\".  \n",
    "‚Ä¢ Example: odom ‚Üí base_link shows robot pose in odometry frame.  \n",
    "\n",
    "2. TF publishers in this case  \n",
    "‚Ä¢ ros_gz_bridge: bridges Gazebo poses, publishes odom ‚Üí base_link if /tf is bridged.  \n",
    "‚Ä¢ robot_state_publisher: publishes static transforms from URDF (relationship btw base_link ‚Üí sensors, wheels).  \n",
    "\n",
    "3. The problem  \n",
    "‚Ä¢ robot_localization also publishes odom ‚Üí base_link (filtered odometry).  \n",
    "‚Ä¢ Gazebo and robot_localization both publishing the same transform causes conflicts.  \n",
    "‚Ä¢ TF does not allow multiple sources for one transform.  \n",
    "\n",
    "4. The solution  \n",
    "‚Ä¢ Keep only one truth for odom ‚Üí base_link.  \n",
    "‚Ä¢ Trust robot_localization (fused odometry).  \n",
    "‚Ä¢ Stop Gazebo from publishing TF by commenting out:  \n",
    "  \"/tf@tf2_msgs/msg/TFMessage@gz.msgs.Pose_V\"  \n",
    "\n",
    "5. Summary  \n",
    "‚Ä¢ ros_gz_bridge: raw Gazebo poses (disable TF part).  \n",
    "‚Ä¢ robot_state_publisher: static transforms from URDF.  \n",
    "‚Ä¢ robot_localization: fused odometry (final odom ‚Üí base_link).  \n",
    "‚Ä¢ Disable Gazebo‚Äôs TF bridge ‚Üí avoid duplicate odometry transform.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab980600",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# See no info from rqt_tf_tree, but see ros_gz pusblisher using topic info.\n",
    "ros2 launch gazebo_sensors spawn_robot.launch.py\n",
    "\n",
    "ros2 run rqt_tf_tree rqt_tf_tree\n",
    "\n",
    "ros2 topic info /tf --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a58da8",
   "metadata": {},
   "source": [
    "We can see that the yellow (raw) odometry starts drifting away from the corrected one very quickly and we can easily bring the robot into a `special situation` if we drive on a curve and hit the wall. In this case the robot is unable to move and the wheels are slipping. The raw odometry believes from the encoder signals that the robot is still moving on a curve while the odometry after the ekf sensor fusion will believe that the robot moves forward straight. Although none of them are correct, but remember, `neither the IMU and neither the odometry` can tell if the robot is doing an `uniform movement` or it's `stand still`. At least the ekf is able to properly tell that the robot's orientation is not changing regardless what the encoders measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Navigation's Concepts for GPS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331df91d",
   "metadata": {},
   "source": [
    "**Latitude (œÜ) and Longitude (Œª)**\n",
    "- Latitude (œÜ): north‚Äìsouth position (‚àí90¬∞ = South Pole, 0¬∞ = Equator, +90¬∞ = North Pole). y-axis\n",
    "- Longitude (Œª): east‚Äìwest position (‚àí180¬∞ to +180¬∞). x-axis\n",
    "\n",
    "**Haversine**\n",
    "- This combines latitude/longitude differences into a value proportional to the chord length between the two points (straight line through Earth).\n",
    "\n",
    "**Central Angle C**\n",
    "- What: angle at the Earth‚Äôs center between the two locations.\n",
    "\n",
    "- Why useful: multiplying this angle by Earth‚Äôs radius ùëÖ gives the `arc length` (distance along Earth‚Äôs surface).\n",
    "That‚Äôs the shortest path (great-circle distance) between two points on a sphere.\n",
    "\n",
    "- Formula:\n",
    "    - a = sin¬≤(ŒîœÜ/2) + cos(œÜ1) * cos(œÜ2) * sin¬≤(ŒîŒª/2) (haversine)\n",
    "    - c = 2 * atan2(‚àöa, ‚àö(1‚àía)) (central angle)\n",
    "    - d = R * c (arc length = distance)\n",
    "Where:\n",
    "- œÜ1, œÜ2 = latitudes of point 1 and 2 (in radians)\n",
    "- Œª1, Œª2 = longitudes of point 1 and 2 (in radians)\n",
    "- ŒîœÜ = œÜ2 ‚àí œÜ1\n",
    "- ŒîŒª = Œª2 ‚àí Œª1\n",
    "- R = Earth‚Äôs radius (mean radius = 6,371 km)\n",
    "\n",
    "**Bearing (Initial Heading, Œ∏)**\n",
    "- The initial heading you must take at Point 1 to travel along the shortest path (great circle) to Point 2.\n",
    "- Units: degrees (0¬∞ = North, 90¬∞ = East, 180¬∞ = South, 270¬∞ = West).\n",
    "- Used in navigation systems, autopilot, robotics for orientation.\n",
    "\n",
    "Here Arc means inverse trigonometry; arcsin(0.5)=30‚àò, because¬†sin(30‚àò)=0.5\n",
    "\n",
    "\n",
    "Summary:\n",
    "- The `Haversine formula` computes `great-circle distance` (shortest path on a sphere) from latitude/longitude, which is more accurate than flat-Earth approximations.\n",
    "- The `central angle` gives a numerically stable way to derive `arc length`, even for very small or very large distances.\n",
    "- The initial `bearing` (azimuth) provides the correct navigation `heading` between two global coordinates.\n",
    "- These calculations are efficient (just trig + sqrt), making them practical for GPS, GIS, aviation, shipping, and robotics.\n",
    "\n",
    "The `haversine + bearing` method is still standard today in navigation and mapping software, though higher-precision ellipsoidal models (like Vincenty) are sometimes used for long distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f0268",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "![alt text](assets/g1.png)\n",
    "![alt text](assets/g2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b6037",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# to view gps mapping\n",
    "\n",
    "ros2 launch gazebo_sensors spawn_robot.launch.py world:=empty.sdf rviz_config:=gps.rviz x:=0.0 y:=0.0 yaw:=0.0\n",
    "\n",
    "\n",
    "# to navigate gps between 4 points and then stopping:\n",
    "ros2 run gazebo_sensors_py gps_waypoint_follower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a671733",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebd67d",
   "metadata": {},
   "source": [
    "**Lidar**\n",
    "\n",
    "According to the different ranging methods, LiDAR is mainly divided into ToF (Time of Flight) and FMCW (Frequency Modulated Continuous Wave) types, and ToF type is the vast majority of LIDARs currently in mass production.\n",
    "\n",
    "1. In the `ToF` (Time of Flight) method (OS1), the LiDAR transmitter emits a pulse, hits an object and returns, and the receiver receives the return wave and calculates the difference in reception time between the two, and multiplies it by the speed of light to achieve distance measurement between objects.\n",
    "    - d=c‚ãÖt‚Äã/2\n",
    "    - Where d is the distance between the object and the LiDAR, c is the speed of light, and t is the round-trip time of the pulse (ToF).\n",
    "    - Pros: simple principle, long range (up to hundreds of meters).\n",
    "    - Cons: lower accuracy (cm level), timing precision limits accuracy\n",
    "\n",
    "2. Phase-Shift LiDAR\n",
    "Emit continuous modulated laser (sinusoidal intensity). Measure phase difference Œîùúô between sent and received signal.\n",
    "    - Distance: ùëë=Œîùúô‚ãÖùúÜ/4ùúã\n",
    "    - Where ùúÜ is the modulation wavelength (speed of light / modulation frequency). \n",
    "    - Pros: very accurate for short‚Äìmedium range.\n",
    "    - Cons: max range limited (phase ambiguity).\n",
    "\n",
    "3. Coherent methods are also used, i.e., for `frequency modulated continuous wave` (FMCW: Aeva, Aurora) LiDAR transmitting a continuous beam with a frequency that varies steadily over time. Since the frequency of the source beam is constantly changing, differences in beam transmission distance result in differences in frequency. After mixing the echo signal with the local oscillation signal and low-pass filtering, the resulting differential frequency signal is a function of the beam round-trip time. The FM continuous wave LiDAR will not be interfered by other LiDAR or sunlight and has no range blindness; it can also measure the speed and distance of objects using Doppler shift. The FM continuous wave LiDAR concept is not new, but faces a number of technical challenges, such as the linewidth limitation of the emitted laser, the frequency range of the linear FM pulses, the linearity of the linear pulse frequency variation, and the reproducibility of individual linear FM pulses.\n",
    "    - Emit chirped (linearly frequency-swept) laser.\n",
    "    - Beat frequency Œîf between sent and received gives delay ‚Üí distance:ùëë=ùëê‚ãÖŒîùëì/2‚ãÖùëò\n",
    "    - Where ùëò = chirp rate.\n",
    "    - Bonus: Doppler shift also gives velocity.\n",
    "    - Pros: range + velocity, very resistant to interference.\n",
    "    - Cons: complex, costlier, still emerging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42d91d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "\n",
    "<img src=\"assets/l1.png\" alt=\"alt text\" width=\"500\" height=\"400\">\n",
    "<img src=\"assets/l2.png\" alt=\"alt text\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c3a7a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Gazebo with the `Visualize Lidar` tool from plugins.\n",
    "- Increase decay time of visualization of lidar scans to do a simple mapping of environment. (from 0 to 30)\n",
    "\n",
    "![alt text](assets/l3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff320873",
   "metadata": {},
   "source": [
    "3D Lidar\n",
    "\n",
    "If we want to simulate a 3D lidar we only have to `increase` the `number of vertical samples` together with the minimum and maximum angles. For example the following vertical parameters are matching a Velodyne VLP-32 sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb8dce",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "          <vertical>\n",
    "              <samples>32</samples>\n",
    "              <min_angle>-0.5353</min_angle>\n",
    "              <max_angle>0.1862</max_angle>\n",
    "          </vertical>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f94dc",
   "metadata": {},
   "source": [
    "To properly visualize a 3D point cloud in RViz we have to forward one more topic with parameter_bridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da87474",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\"/scan/points@sensor_msgs/msg/PointCloud2@gz.msgs.PointCloudPacked\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9867a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "RGBD camera\n",
    "\n",
    "An RGBD camera combines a standard `RGB camera` with a `depth sensor` to provide both `color images` and `depth information` for each pixel. This allows the camera to capture `3D information` about the environment, which is useful for robotics applications like mapping, navigation, and object recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e7bdd",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    " <gazebo reference=\"camera_lens_link\"><!--changed-->\n",
    "    <sensor name=\"rgbd_camera\" type=\"rgbd_camera\">\n",
    "      <camera>\n",
    "        <horizontal_fov>1.25</horizontal_fov>\n",
    "        <image>\n",
    "          <width>320</width>\n",
    "          <height>240</height>\n",
    "        </image>\n",
    "        <clip>\n",
    "          <near>0.3</near>\n",
    "          <far>15</far>\n",
    "        </clip>\n",
    "        <optical_frame_id>camera_link_optical</optical_frame_id>\n",
    "      </camera>\n",
    "      <always_on>1</always_on>\n",
    "      <update_rate>20</update_rate>\n",
    "      <visualize>true</visualize>\n",
    "      <topic>camera</topic>\n",
    "      <gz_frame_id>camera_link</gz_frame_id>\n",
    "    </sensor>\n",
    "  </gazebo>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dc2bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Adds-on Sensors  ([link](https://github.com/gazebosim/gz-sim/tree/gz-sim8/examples/worlds))\n",
    "\n",
    "| Sensor                            | Example                                         | Use                                                              |\n",
    "| --------------------------------- | ----------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Contact sensor**                | `contact_sensor.sdf`                            | Detects physical collisions                                      |\n",
    "| **Thermal / segmentation camera** | `thermal_camera.sdf`, `segmentation_camera.sdf` | Advanced vision (CV/AI)                                          |\n",
    "| **Environmental sensor**          | `environmental_sensor.sdf`                      | Temperature, humidity‚Äîuseful for multi-sensor fusion experiments |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b4bab",
   "metadata": {},
   "source": [
    "Image Processing with OpenCV\n",
    "\n",
    "- Use conventional camera for opencv tasks!\n",
    "\n",
    "- If we want to use OpenCV and other python modules from a python virtual environment, we'll have to add the following to the `setup.cfg` file inside our python package:\n",
    "```bash\n",
    "[build_scripts]\n",
    "executable = /usr/bin/env python3\n",
    "```\n",
    "\n",
    "- Writing an Open cv node that `subscribes` to /camera/image topic, `converts` it to OpenCV compatible frame and `displays` it using OpenCV's imshow function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when process_image() function starts doing `heavier image processing` (like OpenCV computations), it‚Äôll take more time to run. If it runs in the `same thread` as `rclpy.spin_once()`, it will `block ROS from receiving new messages` (like the next camera frame).\n",
    "\n",
    "So, the solution is to move the ROS spin loop into a separate thread, letting one thread handle incoming images continuously while another handles image processing ‚Äî keeping the node responsive and real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa3533",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e59acf",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "  <!-- STEP 7 - Camera -->\n",
    "  <!--new after body update, no camera set-->\n",
    "  \n",
    "  <joint type=\"fixed\" name=\"camera_joint\">\n",
    "    <origin xyz=\"0.220 0 0.075\" rpy=\"0 0 0\"/>\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child  link=\"camera_link\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_link\">\n",
    "    <inertial>\n",
    "      <mass value=\"0.1\"/>\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <inertia ixx=\"1e-6\" ixy=\"0\" ixz=\"0\" iyy=\"1e-6\" iyz=\"0\" izz=\"1e-6\"/>\n",
    "    </inertial>\n",
    "\n",
    "    <!-- body box that is only decorative and collision -->\n",
    "    <collision name=\"camera_body_collision\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <geometry><box size=\"0.04 0.03 0.03\"/></geometry>\n",
    "    </collision>\n",
    "\n",
    "    <visual name=\"camera_body_visual\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n",
    "      <geometry><box size=\"0.04 0.06 0.04\"/></geometry>\n",
    "      <material name=\"grey\"/>\n",
    "    </visual>\n",
    "  </link>\n",
    "\n",
    "  <!-- separate lens link positioned forward of the box -->\n",
    "  <joint type=\"fixed\" name=\"camera_lens_joint\">\n",
    "    <parent link=\"camera_link\"/>\n",
    "    <!-- put lens where you want the optical center to be -->\n",
    "    <origin xyz=\"0.025 0 0\" rpy=\"0 0 0\"/>\n",
    "    <child  link=\"camera_lens_link\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_lens_link\">\n",
    "    <!-- lens visual placed at this link origin -->\n",
    "    <visual name=\"camera_lens_visual\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 1.5707 0\"/>\n",
    "      <geometry>\n",
    "        <cylinder radius=\"0.015\" length=\"0.03\"/>\n",
    "      </geometry>\n",
    "      <material name=\"black\"/>\n",
    "    </visual>\n",
    "  </link>\n",
    "\n",
    "  <!-- optical frame now coincides with lens link origin -->\n",
    "  <joint type=\"fixed\" name=\"camera_optical_joint\">\n",
    "    <parent link=\"camera_lens_link\"/>\n",
    "    <child  link=\"camera_link_optical\"/>\n",
    "    <!-- optical joint placed at lens center so sensor uses lens link as reference -->\n",
    "    <origin xyz=\"0 0 0\" rpy=\"-1.5707 0 -1.5707\"/>\n",
    "  </joint>\n",
    "\n",
    "  <link name=\"camera_link_optical\"></link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c0e1d",
   "metadata": {},
   "source": [
    "\n",
    "To do:\n",
    "1. Draw a slide on EKF Sensor fusion, equations, usecase in robotics. Then slide on diagram on Drift in odometry vs filtered ekf odometery (green vs yellow output)\n",
    "2. Differential Drive Equations slide and Modelling\n",
    "3. Sensors slide (with IMU, Lidar, Camera, GPS)- 3x slides with 1 focusing on point cloud of lidar and camera depth image\n",
    "4. Image processing with opencv\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
